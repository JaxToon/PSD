{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67064c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb Movie Review Scraper (2025 Updated Version)\n",
      "-----------------------------------------------\n",
      "Note: This script requires Chrome and chromedriver to be installed.\n",
      "It will automatically download chromedriver if not already installed.\n",
      "Initial setup may take a moment.\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 13:52:13,250 - INFO - \n",
      "Searching for movie: Avatar\n",
      "2025-04-24 13:52:13,251 - INFO - ====== WebDriver manager ======\n",
      "2025-04-24 13:52:14,632 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-04-24 13:52:14,732 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-04-24 13:52:14,826 - INFO - Driver [C:\\Users\\LEGION\\.wdm\\drivers\\chromedriver\\win64\\135.0.7049.114\\chromedriver-win32/chromedriver.exe] found in cache\n",
      "2025-04-24 13:52:16,062 - INFO - Searching with URL: https://www.imdb.com/find/?q=Avatar&s=tt&exact=true\n",
      "2025-04-24 13:52:27,807 - INFO - Screenshot saved to debug_search_results_20250424_135227.png\n",
      "2025-04-24 13:52:28,045 - INFO - Found 25 results using selector: .find-result-item\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found the following movies:\n",
      "1. Avatar (Unknown) - tt0499549\n",
      "2. Avatar: The Last Airbender (Unknown) - tt0417299\n",
      "3. Tomorrow Never Dies (Unknown) - tt0120347\n",
      "4. Avatar (Unknown) - tt5863892\n",
      "5. Avatar (Unknown) - tt27931855\n",
      "6. Avatar (Unknown) - tt1775309\n",
      "7. Avatar (Unknown) - tt0278325\n",
      "8. Avatar (Unknown) - tt0154182\n",
      "9. Avatar (Unknown) - tt0375570\n",
      "10. Chrysalis (Unknown) - tt0884335\n",
      "11. Avatar (Unknown) - tt1622577\n",
      "12. Avatar (Unknown) - tt0959431\n",
      "13. Cyber Wars (Unknown) - tt0270841\n",
      "14. Avatar (Unknown) - tt0497595\n",
      "15. Rifftrax: Avatar (Unknown) - tt16492516\n",
      "16. Avatar (Unknown) - tt1378189\n",
      "17. Avatar (Unknown) - tt21833600\n",
      "18. Avatar (Unknown) - tt2136754\n",
      "19. Avatar (Unknown) - tt0709042\n",
      "20. Avatar (Unknown) - tt32623861\n",
      "21. Avatar (Unknown) - tt0751080\n",
      "22. Avatar (Unknown) - tt0860057\n",
      "23. Avatar (Unknown) - tt1015442\n",
      "24. Avatar (Unknown) - tt0703664\n",
      "25. Avatar (Unknown) - tt10932508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 13:52:33,842 - INFO - \n",
      "Scraping reviews for: Avatar (tt0499549)\n",
      "2025-04-24 13:52:33,844 - INFO - Opening reviews page: https://www.imdb.com/title/tt0499549/reviews\n",
      "2025-04-24 13:52:54,042 - INFO - Screenshot saved to debug_reviews_initial_tt0499549_20250424_135253.png\n",
      "2025-04-24 13:52:56,367 - INFO - Saved debug HTML to debug_imdb_tt0499549.html\n",
      "2025-04-24 13:52:56,371 - INFO - Found main review section container\n",
      "2025-04-24 13:52:56,380 - INFO - Found 25 article elements that might contain reviews\n",
      "2025-04-24 13:52:56,477 - INFO - Found 24 reviews on initial page\n",
      "2025-04-24 13:52:57,629 - INFO - Clicked 'Load More' button using selector: button.ipc-see-more__button\n",
      "2025-04-24 13:53:03,811 - INFO - Screenshot saved to debug_reviews_more_tt0499549_page1_20250424_135303.png\n",
      "2025-04-24 13:53:04,333 - INFO - Saved debug HTML to debug_imdb_tt0499549.html\n",
      "2025-04-24 13:53:04,338 - INFO - Found main review section container\n",
      "2025-04-24 13:53:04,353 - INFO - Found 50 article elements that might contain reviews\n",
      "2025-04-24 13:53:04,553 - INFO - Loaded page 2: Found 24 new reviews (Total: 48)\n",
      "2025-04-24 13:53:04,555 - INFO - \n",
      "Found 48 reviews for Avatar\n",
      "2025-04-24 13:53:04,561 - INFO - \n",
      "Reviews saved to reviews/reviews_tt0499549_Avatar.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException\n",
    "import logging\n",
    "import base64\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def initialize_driver():\n",
    "    \"\"\"Initialize and return a Selenium WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    \n",
    "    # Randomize user agent to avoid detection\n",
    "    user_agents = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/109.0\",\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"\n",
    "    ]\n",
    "    \n",
    "    chrome_options.add_argument(f\"--user-agent={random.choice(user_agents)}\")\n",
    "    \n",
    "    # Additional options to avoid detection\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "    \n",
    "    try:\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "        # Execute CDP command to bypass bot detection\n",
    "        driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n",
    "            \"source\": \"\"\"\n",
    "                Object.defineProperty(navigator, 'webdriver', {\n",
    "                    get: () => undefined\n",
    "                });\n",
    "                \n",
    "                // Additional stealth setup\n",
    "                const originalQuery = window.navigator.permissions.query;\n",
    "                window.navigator.permissions.query = (parameters) => (\n",
    "                    parameters.name === 'notifications' ?\n",
    "                        Promise.resolve({state: Notification.permission}) :\n",
    "                        originalQuery(parameters)\n",
    "                );\n",
    "            \"\"\"\n",
    "        })\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize driver: {e}\")\n",
    "        raise\n",
    "\n",
    "def random_delay(min_seconds=2, max_seconds=5):\n",
    "    \"\"\"Add a random delay between requests to avoid detection\"\"\"\n",
    "    delay = random.uniform(min_seconds, max_seconds)\n",
    "    time.sleep(delay)\n",
    "    return delay\n",
    "\n",
    "def take_screenshot(driver, name=\"screenshot\"):\n",
    "    \"\"\"Save a screenshot for debugging purposes\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"debug_{name}_{timestamp}.png\"\n",
    "    driver.save_screenshot(filename)\n",
    "    logger.info(f\"Screenshot saved to {filename}\")\n",
    "\n",
    "def search_movie_by_title(movie_title, driver):\n",
    "    \"\"\"Search for a movie by title using Selenium WebDriver\"\"\"\n",
    "    search_url = f\"https://www.imdb.com/find/?q={movie_title.replace(' ', '+')}&s=tt&exact=true\"\n",
    "    \n",
    "    logger.info(f\"Searching with URL: {search_url}\")\n",
    "    \n",
    "    try:\n",
    "        # Visit the IMDb homepage first to get cookies\n",
    "        driver.get(\"https://www.imdb.com/\")\n",
    "        random_delay(2, 4)\n",
    "        \n",
    "        # Now navigate to the search URL\n",
    "        driver.get(search_url)\n",
    "        random_delay(3, 7)\n",
    "        \n",
    "        # Take a screenshot of search results\n",
    "        take_screenshot(driver, \"search_results\")\n",
    "        \n",
    "        # Wait for search results to appear - updated selectors for 2025 IMDb\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \".find-result-item, .ipc-metadata-list-summary-item\"))\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            logger.warning(\"Timeout waiting for search results. Checking for any results format.\")\n",
    "\n",
    "        # Parse the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Save HTML for debugging\n",
    "        with open(f\"debug_search_{movie_title.replace(' ', '_')}.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(soup.prettify())\n",
    "        \n",
    "        # Find movie results\n",
    "        search_results = []\n",
    "        \n",
    "        # Try multiple possible selectors for results\n",
    "        result_selectors = [\n",
    "            '.find-result-item',                # Current IMDb (2025)\n",
    "            '.ipc-metadata-list-summary-item',  # Alternative format\n",
    "            '.findResult',                      # Legacy format\n",
    "            'li.ipc-list__item'                 # Generic list items\n",
    "        ]\n",
    "        \n",
    "        result_items = []\n",
    "        for selector in result_selectors:\n",
    "            result_items = soup.select(selector)\n",
    "            if result_items:\n",
    "                logger.info(f\"Found {len(result_items)} results using selector: {selector}\")\n",
    "                break\n",
    "        \n",
    "        if not result_items:\n",
    "            # Try alternative search format\n",
    "            alternative_url = f\"https://www.imdb.com/search/title/?title={movie_title.replace(' ', '+')}\"\n",
    "            logger.info(f\"No results found. Trying alternative search: {alternative_url}\")\n",
    "            \n",
    "            driver.get(alternative_url)\n",
    "            random_delay(3, 5)\n",
    "            \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            result_items = soup.select('.lister-item')\n",
    "        \n",
    "        # Process search results\n",
    "        for item in result_items:\n",
    "            try:\n",
    "                # Find the title link - try multiple possible selectors\n",
    "                link = (\n",
    "                    item.select_one('a[href*=\"/title/tt\"]') or \n",
    "                    item.select_one('.ipc-metadata-list-summary-item__t') or\n",
    "                    item.select_one('.result_text a') or\n",
    "                    item.select_one('a[data-testid=\"title\"]')\n",
    "                )\n",
    "                \n",
    "                if not link:\n",
    "                    continue\n",
    "                \n",
    "                title = link.text.strip()\n",
    "                href = link.get('href', '')\n",
    "                \n",
    "                # Extract IMDb ID\n",
    "                imdb_id_match = re.search(r'/title/(tt\\d+)/?', href)\n",
    "                if not imdb_id_match:\n",
    "                    continue\n",
    "                \n",
    "                imdb_id = imdb_id_match.group(1)\n",
    "                \n",
    "                # Try to extract year using various selectors and patterns\n",
    "                year = \"Unknown\"\n",
    "                # Look for year in the item text\n",
    "                year_matches = re.findall(r'\\((\\d{4})\\)', item.text)\n",
    "                if year_matches:\n",
    "                    year = year_matches[0]\n",
    "                \n",
    "                search_results.append({\n",
    "                    'title': title,\n",
    "                    'year': year,\n",
    "                    'imdb_id': imdb_id\n",
    "                })\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error parsing result: {e}\")\n",
    "        \n",
    "        return search_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during search: {e}\")\n",
    "        return []\n",
    "\n",
    "def diagnose_review_page(page_source, imdb_id):\n",
    "    \"\"\"Output diagnostic information about the page structure\"\"\"\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    logger.info(\"------- DIAGNOSTIC INFO -------\")\n",
    "    \n",
    "    # Save full HTML for inspection\n",
    "    debug_file = f\"debug_fullpage_{imdb_id}.html\"\n",
    "    with open(debug_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(soup.prettify())\n",
    "    logger.info(f\"Saved full HTML to {debug_file}\")\n",
    "    \n",
    "    # Check for specific 2025 IMDb review section structure\n",
    "    review_section = soup.select_one(\"section.ipc-page-section--sp-pageMargin\")\n",
    "    if review_section:\n",
    "        logger.info(\"Found main review section container\")\n",
    "        \n",
    "        # Look for article elements which should contain reviews\n",
    "        articles = review_section.select(\"article\")\n",
    "        logger.info(f\"Found {len(articles)} article elements in review section\")\n",
    "        \n",
    "        # Examine first few articles\n",
    "        for i, article in enumerate(articles[:3]):\n",
    "            logger.info(f\"Article {i+1} classes: {article.get('class', 'none')}\")\n",
    "            logger.info(f\"Article {i+1} contains {len(article.select('div'))} div elements\")\n",
    "            \n",
    "            # Check for review card structure\n",
    "            review_card = article.select_one(\".ipc-list-card\")\n",
    "            if review_card:\n",
    "                logger.info(f\"Article {i+1} contains review card structure\")\n",
    "                \n",
    "                # Check for review content\n",
    "                review_content = article.select_one(\".review-content\")\n",
    "                if review_content:\n",
    "                    logger.info(f\"Found review content: {review_content.text[:50]}...\")\n",
    "                else:\n",
    "                    logger.info(\"No specific review content class found\")\n",
    "                    \n",
    "                # Check for HTML content\n",
    "                html_content = article.select_one(\".ipc-html-content\")\n",
    "                if html_content:\n",
    "                    logger.info(f\"Found HTML content section: {html_content.text[:50]}...\")\n",
    "    else:\n",
    "        logger.warning(\"Could not find main review section container\")\n",
    "    \n",
    "    # Check for common review indicators\n",
    "    potential_patterns = [\n",
    "        'user review', 'out of 10', 'rated this', 'review title', \n",
    "        'spoiler alert', 'was this review helpful'\n",
    "    ]\n",
    "    \n",
    "    for pattern in potential_patterns:\n",
    "        matches = soup.find_all(string=re.compile(pattern, re.IGNORECASE))\n",
    "        if matches:\n",
    "            logger.info(f\"Found {len(matches)} elements containing '{pattern}'\")\n",
    "    \n",
    "    # Check for pagination or load more elements\n",
    "    load_more = soup.select('button.ipc-see-more__button, [data-testid=\"load-more\"], .load-more')\n",
    "    if load_more:\n",
    "        logger.info(f\"Found {len(load_more)} potential 'load more' elements\")\n",
    "        for elem in load_more[:2]:\n",
    "            logger.info(f\"Load more element: {elem.get('class', 'none')}, text: {elem.text.strip()}\")\n",
    "    \n",
    "    logger.info(\"------- END DIAGNOSTIC INFO -------\")\n",
    "\n",
    "def extract_review_from_article(article):\n",
    "    \"\"\"Extract review details from a 2025 IMDb article element\"\"\"\n",
    "    review_data = {}\n",
    "    \n",
    "    try:\n",
    "        # Extract reviewer name and URL from the user info section\n",
    "        user_info = article.select_one(\"ul li a\")\n",
    "        if user_info:\n",
    "            review_data['reviewer_name'] = user_info.text.strip()\n",
    "            review_data['reviewer_url'] = user_info.get('href', '')\n",
    "        \n",
    "        # Extract review date\n",
    "        date_element = article.select_one(\"ul li:nth-child(2)\")\n",
    "        if date_element:\n",
    "            review_data['review_date'] = date_element.text.strip()\n",
    "        \n",
    "        # Extract rating if available\n",
    "        rating_element = article.select_one(\"span.ipc-rating-star\")\n",
    "        if rating_element:\n",
    "            rating_text = rating_element.text.strip()\n",
    "            rating_match = re.search(r'(\\d+(?:\\.\\d+)?)', rating_text)\n",
    "            if rating_match:\n",
    "                review_data['rating_value'] = rating_match.group(1)\n",
    "        \n",
    "        # Extract short review (title)\n",
    "        title_element = article.select_one(\"div.edCLQz > div\")\n",
    "        if title_element:\n",
    "            review_data['short_review'] = title_element.text.strip()\n",
    "        \n",
    "        # Try multiple selectors for full review content\n",
    "        content_element = None\n",
    "        \n",
    "        # First try the precise selector provided\n",
    "        content_element = article.select_one(\"div.ipc-list-card--border-speech div.ipc-list-card__content div:nth-child(3) div.ipc-html-content.ipc-html-content--base.review-content div\")\n",
    "        \n",
    "        # If that fails, try the original selector\n",
    "        if not content_element:\n",
    "            content_element = article.select_one(\".ipc-html-content.review-content\")\n",
    "            \n",
    "        # Additional fallback selector\n",
    "        if not content_element:\n",
    "            content_element = article.select_one(\"div.ipc-html-content--base\")\n",
    "        \n",
    "        if content_element:\n",
    "            review_data['full_review'] = content_element.text.strip()\n",
    "        \n",
    "        # Extract review ID\n",
    "        review_data['data-review-id'] = article.get('id', '')\n",
    "        \n",
    "        # Check if we got meaningful data\n",
    "        if not (review_data.get('short_review') or review_data.get('full_review')):\n",
    "            logger.debug(\"Could not extract essential review text\")\n",
    "            return None\n",
    "            \n",
    "        return review_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting review from article: {e}\")\n",
    "        return None\n",
    "    \n",
    "def scrape_reviews_page_2025(page_source, imdb_id):\n",
    "    \"\"\"Extract reviews from the 2025 IMDb page structure\"\"\"\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    # Debug: Save the HTML to examine the structure\n",
    "    with open(f\"debug_imdb_{imdb_id}.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(soup.prettify())\n",
    "    \n",
    "    logger.info(f\"Saved debug HTML to debug_imdb_{imdb_id}.html\")\n",
    "    \n",
    "    # First try to find the main review section\n",
    "    review_section = soup.select_one(\"section.ipc-page-section--sp-pageMargin\")\n",
    "    \n",
    "    reviews_data = {\n",
    "        'ImdbId': imdb_id,\n",
    "        'reviews': []\n",
    "    }\n",
    "    \n",
    "    if review_section:\n",
    "        logger.info(\"Found main review section container\")\n",
    "        \n",
    "        # Find all article elements which contain individual reviews\n",
    "        articles = review_section.select(\"article\")\n",
    "        logger.info(f\"Found {len(articles)} article elements that might contain reviews\")\n",
    "        \n",
    "        for article in articles:\n",
    "            review_data = extract_review_from_article(article)\n",
    "            if review_data:\n",
    "                reviews_data['reviews'].append(review_data)\n",
    "    \n",
    "    if not reviews_data['reviews']:\n",
    "        logger.warning(\"Could not find reviews using 2025 structure. Running diagnostics...\")\n",
    "        diagnose_review_page(page_source, imdb_id)\n",
    "        \n",
    "        # Fallback to generic scraping approach\n",
    "        return scrape_reviews_page_generic(page_source, imdb_id)\n",
    "    \n",
    "    return reviews_data\n",
    "\n",
    "def scrape_reviews_page_generic(page_source, imdb_id):\n",
    "    \"\"\"Generic fallback method to find review-like content\"\"\"\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    reviews_data = {\n",
    "        'ImdbId': imdb_id,\n",
    "        'reviews': []\n",
    "    }\n",
    "    \n",
    "    # Look for elements that might contain reviews based on content\n",
    "    potential_reviews = []\n",
    "    \n",
    "    # Look for divs with substantial text content\n",
    "    for div in soup.find_all(['div', 'article', 'section']):\n",
    "        text = div.text.strip()\n",
    "        if len(text) > 200:  # Minimum length for review content\n",
    "            # Check if it has review-like content\n",
    "            lower_text = text.lower()\n",
    "            if any(marker in lower_text for marker in ['review', 'rating', 'stars', 'out of 10', 'recommended']):\n",
    "                potential_reviews.append(div)\n",
    "    \n",
    "    logger.info(f\"Found {len(potential_reviews)} potential review-like elements\")\n",
    "    \n",
    "    # Process these potential reviews\n",
    "    for i, element in enumerate(potential_reviews):\n",
    "        try:\n",
    "            review_data = {}\n",
    "            \n",
    "            # Try to extract review title\n",
    "            title_candidates = element.find_all(['h3', 'h4', 'strong', 'b'])\n",
    "            if title_candidates:\n",
    "                review_data['short_review'] = title_candidates[0].text.strip()\n",
    "            \n",
    "            # Extract main text content\n",
    "            paragraphs = element.find_all('p')\n",
    "            if paragraphs:\n",
    "                review_data['full_review'] = ' '.join([p.text.strip() for p in paragraphs])\n",
    "            else:\n",
    "                # If no paragraphs, use the element's text\n",
    "                review_data['full_review'] = element.text.strip()\n",
    "            \n",
    "            # Look for rating patterns\n",
    "            rating_match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*(?:/\\s*10|stars?)', element.text)\n",
    "            if rating_match:\n",
    "                review_data['rating_value'] = rating_match.group(1)\n",
    "            \n",
    "            # Look for date patterns\n",
    "            date_match = re.search(r'\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{4}', element.text)\n",
    "            if date_match:\n",
    "                review_data['review_date'] = date_match.group(0)\n",
    "            \n",
    "            # Only add if we have meaningful content\n",
    "            if review_data.get('full_review') and len(review_data['full_review']) > 50:\n",
    "                # Create a unique ID\n",
    "                review_data['data-review-id'] = f\"generic_{i}\"\n",
    "                reviews_data['reviews'].append(review_data)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing potential review: {e}\")\n",
    "    \n",
    "    return reviews_data\n",
    "\n",
    "def scroll_page(driver, amount=None):\n",
    "    \"\"\"Scroll the page down to load lazy content\"\"\"\n",
    "    if amount:\n",
    "        driver.execute_script(f\"window.scrollBy(0, {amount});\")\n",
    "    else:\n",
    "        # Scroll to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  # Give time for content to load\n",
    "\n",
    "def click_load_more(driver):\n",
    "    \"\"\"Try to click the 'Load More' button using various approaches\"\"\"\n",
    "    load_more_selectors = [\n",
    "        \"button.ipc-see-more__button\",\n",
    "        \"button.load-more\",\n",
    "        \"[data-testid='load-more']\",\n",
    "        \".ipc-pagination__next-button\",\n",
    "        \".see-more button\",\n",
    "        \".ipl-load-more__button\"\n",
    "    ]\n",
    "    \n",
    "    for selector in load_more_selectors:\n",
    "        try:\n",
    "            # Try to find the button\n",
    "            load_more = WebDriverWait(driver, 5).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, selector))\n",
    "            )\n",
    "            \n",
    "            # Scroll to make button visible\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});\", load_more)\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Try to click\n",
    "            try:\n",
    "                load_more.click()\n",
    "                logger.info(f\"Clicked 'Load More' button using selector: {selector}\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                # If normal click fails, try JavaScript click\n",
    "                logger.info(f\"Normal click failed: {e}. Trying JavaScript click.\")\n",
    "                driver.execute_script(\"arguments[0].click();\", load_more)\n",
    "                logger.info(f\"Used JavaScript to click 'Load More' button\")\n",
    "                return True\n",
    "                \n",
    "        except TimeoutException:\n",
    "            continue\n",
    "    \n",
    "    # If all selectors failed, try using JavaScript to find and click any button that looks like \"Load More\"\n",
    "    try:\n",
    "        clicked = driver.execute_script(\"\"\"\n",
    "            // Find buttons with \"load more\" or \"next\" text\n",
    "            var buttons = Array.from(document.querySelectorAll('button, [role=\"button\"], a.ipc-button'));\n",
    "            \n",
    "            for (var i = 0; i < buttons.length; i++) {\n",
    "                var btn = buttons[i];\n",
    "                var text = btn.textContent.toLowerCase();\n",
    "                \n",
    "                if (text.includes('load more') || text.includes('show more') || text.includes('next')) {\n",
    "                    btn.scrollIntoView({behavior: 'smooth', block: 'center'});\n",
    "                    setTimeout(() => {}, 500);\n",
    "                    btn.click();\n",
    "                    return true;\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return false;\n",
    "        \"\"\")\n",
    "        \n",
    "        if clicked:\n",
    "            logger.info(\"Found and clicked 'Load More' button using JavaScript\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"JavaScript click attempt failed: {e}\")\n",
    "    \n",
    "    logger.info(\"No 'Load More' button found or clickable\")\n",
    "    return False\n",
    "\n",
    "def scrape_all_reviews(imdb_id, driver, max_pages=None):\n",
    "    \"\"\"Scrape all review pages for a movie using Selenium with improved 2025 support\"\"\"\n",
    "    reviews_url = f\"https://www.imdb.com/title/{imdb_id}/reviews\"\n",
    "    all_reviews = []\n",
    "    \n",
    "    logger.info(f\"Opening reviews page: {reviews_url}\")\n",
    "    driver.get(reviews_url)\n",
    "    random_delay(3, 7)\n",
    "    \n",
    "    # Accept cookies if the dialog appears\n",
    "    try:\n",
    "        cookie_selectors = [\n",
    "            \"button[id*='accept']\",\n",
    "            \"button[data-testid='accept']\",\n",
    "            \".ipc-button--accept-cookies\",\n",
    "            \".accept-cookies\"\n",
    "        ]\n",
    "        \n",
    "        for selector in cookie_selectors:\n",
    "            try:\n",
    "                accept_button = WebDriverWait(driver, 3).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, selector))\n",
    "                )\n",
    "                accept_button.click()\n",
    "                logger.info(f\"Accepted cookies using selector: {selector}\")\n",
    "                random_delay(1, 2)\n",
    "                break\n",
    "            except TimeoutException:\n",
    "                continue\n",
    "    except Exception:\n",
    "        logger.info(\"No cookie prompt found or already accepted\")\n",
    "\n",
    "    # Take screenshot of initial review page\n",
    "    take_screenshot(driver, f\"reviews_initial_{imdb_id}\")\n",
    "    \n",
    "    # Initial scroll to trigger lazy loading\n",
    "    scroll_page(driver)\n",
    "    \n",
    "    # Get initial reviews\n",
    "    data = scrape_reviews_page_2025(driver.page_source, imdb_id)\n",
    "    if data['reviews']:\n",
    "        all_reviews.extend(data['reviews'])\n",
    "        logger.info(f\"Found {len(data['reviews'])} reviews on initial page\")\n",
    "    else:\n",
    "        logger.warning(\"No reviews found on initial page. Trying alternative approach...\")\n",
    "        \n",
    "        # Try refreshing the page and waiting longer\n",
    "        driver.refresh()\n",
    "        random_delay(5, 10)\n",
    "        scroll_page(driver)\n",
    "        \n",
    "        # Try again\n",
    "        data = scrape_reviews_page_2025(driver.page_source, imdb_id)\n",
    "        if data['reviews']:\n",
    "            all_reviews.extend(data['reviews'])\n",
    "            logger.info(f\"Found {len(data['reviews'])} reviews after refresh\")\n",
    "        else:\n",
    "            logger.warning(\"Still no reviews found. Is the movie very new or has no reviews?\")\n",
    "    \n",
    "    # Keep track of page count and continue loading more reviews if available\n",
    "    page_count = 1\n",
    "    attempt_count = 0\n",
    "    max_attempts = 5  # Maximum attempts to load more content if no new reviews appear\n",
    "    \n",
    "    # Continue loading more reviews until we reach the limit or no more are available\n",
    "    while (max_pages is None or page_count < max_pages) and attempt_count < max_attempts:\n",
    "        # Remember current review count\n",
    "        current_review_count = len(all_reviews)\n",
    "        \n",
    "        # Try to click \"Load More\" button\n",
    "        if click_load_more(driver):\n",
    "            # Wait for new content to load\n",
    "            random_delay(3, 6)\n",
    "            \n",
    "            # Scroll to ensure new content is rendered\n",
    "            scroll_page(driver)\n",
    "            \n",
    "            # Take screenshot after loading more\n",
    "            take_screenshot(driver, f\"reviews_more_{imdb_id}_page{page_count}\")\n",
    "            \n",
    "            # Extract new reviews\n",
    "            data = scrape_reviews_page_2025(driver.page_source, imdb_id)\n",
    "            \n",
    "            # Check if we got new reviews\n",
    "            if data['reviews'] and len(data['reviews']) > current_review_count:\n",
    "                # If we have new reviews, reset attempt counter\n",
    "                new_reviews = data['reviews'][current_review_count:]\n",
    "                all_reviews.extend(new_reviews)\n",
    "                page_count += 1\n",
    "                attempt_count = 0\n",
    "                logger.info(f\"Loaded page {page_count}: Found {len(new_reviews)} new reviews (Total: {len(all_reviews)})\")\n",
    "            else:\n",
    "                # No new reviews, increment attempt counter\n",
    "                attempt_count += 1\n",
    "                logger.info(f\"No new reviews loaded (attempt {attempt_count}/{max_attempts})\")\n",
    "                \n",
    "                # Try scrolling more and waiting\n",
    "                scroll_page(driver)\n",
    "                random_delay(2, 4)\n",
    "        else:\n",
    "            # No more \"Load More\" button found\n",
    "            logger.info(\"No more 'Load More' button found, reached end of reviews\")\n",
    "            break\n",
    "    \n",
    "    # Remove potential duplicates\n",
    "    unique_reviews = []\n",
    "    seen_reviews = set()\n",
    "    \n",
    "    for review in all_reviews:\n",
    "        # Create a hash of review content to identify duplicates\n",
    "        review_hash = hash((review.get('full_review', '') or '') + \n",
    "                           (review.get('short_review', '') or '') + \n",
    "                           (review.get('reviewer_name', '') or ''))\n",
    "        \n",
    "        if review_hash not in seen_reviews:\n",
    "            seen_reviews.add(review_hash)\n",
    "            unique_reviews.append(review)\n",
    "    \n",
    "    if len(unique_reviews) < len(all_reviews):\n",
    "        logger.info(f\"Removed {len(all_reviews) - len(unique_reviews)} duplicate reviews\")\n",
    "    \n",
    "    result = {\n",
    "        'ImdbId': imdb_id,\n",
    "        'total_reviews': len(unique_reviews),\n",
    "        'reviews': unique_reviews\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_movie_reviews_by_title(movie_title, max_pages=None):\n",
    "    \"\"\"Main function to get reviews by movie title using Selenium\"\"\"\n",
    "    logger.info(f\"\\nSearching for movie: {movie_title}\")\n",
    "    \n",
    "    driver = initialize_driver()\n",
    "    \n",
    "    try:\n",
    "        # Search for the movie\n",
    "        search_results = search_movie_by_title(movie_title, driver)\n",
    "        \n",
    "        if not search_results:\n",
    "            logger.warning(\"No movies found matching that title.\")\n",
    "            return None\n",
    "        \n",
    "        # Display search results\n",
    "        print(\"\\nFound the following movies:\")\n",
    "        for i, movie in enumerate(search_results, 1):\n",
    "            print(f\"{i}. {movie['title']} ({movie['year']}) - {movie['imdb_id']}\")\n",
    "        \n",
    "        # Let user choose a movie or use first result in automated mode\n",
    "        selected_movie = None\n",
    "        if len(search_results) == 1:\n",
    "            selected_movie = search_results[0]\n",
    "            print(f\"Auto-selecting the only result: {selected_movie['title']}\")\n",
    "        else:\n",
    "            try:\n",
    "                choice = int(input(f\"\\nSelect a movie (1-{len(search_results)}): \"))\n",
    "                if 1 <= choice <= len(search_results):\n",
    "                    selected_movie = search_results[choice-1]\n",
    "                else:\n",
    "                    logger.error(\"Invalid selection\")\n",
    "                    return None\n",
    "            except ValueError:\n",
    "                logger.error(\"Please enter a valid number\")\n",
    "                return None\n",
    "        \n",
    "        imdb_id = selected_movie['imdb_id']\n",
    "        movie_title = selected_movie['title']\n",
    "        \n",
    "        logger.info(f\"\\nScraping reviews for: {movie_title} ({imdb_id})\")\n",
    "        \n",
    "        # Scrape reviews for the selected movie\n",
    "        data = scrape_all_reviews(imdb_id, driver, max_pages)\n",
    "        \n",
    "        # Count total reviews\n",
    "        total_reviews = len(data['reviews'])\n",
    "        \n",
    "        logger.info(f\"\\nFound {total_reviews} reviews for {movie_title}\")\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(\"reviews\", exist_ok=True)\n",
    "        \n",
    "        # Save to JSON file\n",
    "        sanitized_title = re.sub(r'[\\\\/*?:\"<>|]', \"\", movie_title.replace(' ', '_'))\n",
    "        filename = f\"reviews/reviews_{imdb_id}_{sanitized_title}.json\"\n",
    "        with open(filename, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        logger.info(f\"\\nReviews saved to {filename}\")\n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        return None\n",
    "    finally:\n",
    "        # Always close the driver when done\n",
    "        driver.quit()\n",
    "\n",
    "def main():\n",
    "    print(\"IMDb Movie Review Scraper (2025 Updated Version)\")\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(\"Note: This script requires Chrome and chromedriver to be installed.\")\n",
    "    print(\"It will automatically download chromedriver if not already installed.\")\n",
    "    print(\"Initial setup may take a moment.\")\n",
    "    print(\"-----------------------------------------------\")\n",
    "    \n",
    "    while True:\n",
    "        movie_title = input(\"\\nEnter movie title (or 'quit' to exit): \")\n",
    "        if movie_title.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        max_pages = None\n",
    "        page_limit = input(\"Enter maximum number of pages to scrape (or press Enter for all): \")\n",
    "        if page_limit.strip():\n",
    "            try:\n",
    "                max_pages = int(page_limit)\n",
    "            except ValueError:\n",
    "                print(\"Invalid number, scraping all pages.\")\n",
    "        \n",
    "        get_movie_reviews_by_title(movie_title, max_pages)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
