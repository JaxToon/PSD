{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19318d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.31.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\kampus merdeka mbkm\\company stuff\\.conda\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: requests in d:\\kampus merdeka mbkm\\company stuff\\.conda\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in d:\\kampus merdeka mbkm\\company stuff\\.conda\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.1)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in d:\\kampus merdeka mbkm\\company stuff\\.conda\\lib\\site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in d:\\kampus merdeka mbkm\\company stuff\\.conda\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Collecting websocket-client~=1.8 (from selenium)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: python-dotenv in d:\\kampus merdeka mbkm\\company stuff\\.conda\\lib\\site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: packaging in d:\\kampus merdeka mbkm\\company stuff\\.conda\\lib\\site-packages (from webdriver-manager) (23.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\kampus merdeka mbkm\\company stuff\\.conda\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\kampus merdeka mbkm\\company stuff\\.conda\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\kampus merdeka mbkm\\company stuff\\.conda\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: attrs>=23.2.0 in d:\\kampus merdeka mbkm\\company stuff\\.conda\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in d:\\kampus merdeka mbkm\\company stuff\\.conda\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in d:\\kampus merdeka mbkm\\company stuff\\.conda\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pycparser in d:\\kampus merdeka mbkm\\company stuff\\.conda\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in d:\\kampus merdeka mbkm\\company stuff\\.conda\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Downloading selenium-4.31.0-py3-none-any.whl (9.4 MB)\n",
      "   ---------------------------------------- 0.0/9.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/9.4 MB 3.7 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.9/9.4 MB 9.6 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.6/9.4 MB 11.3 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.7/9.4 MB 14.2 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.7/9.4 MB 15.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.3/9.4 MB 15.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.2/9.4 MB 15.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.1/9.4 MB 16.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.1/9.4 MB 16.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.1/9.4 MB 17.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.2/9.4 MB 17.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.4/9.4 MB 17.1 MB/s eta 0:00:00\n",
      "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Downloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
      "   ---------------------------------------- 0.0/499.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 499.2/499.2 kB 15.8 MB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.8/58.8 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: sortedcontainers, wsproto, websocket-client, pysocks, outcome, webdriver-manager, trio, trio-websocket, selenium\n",
      "Successfully installed outcome-1.3.0.post0 pysocks-1.7.1 selenium-4.31.0 sortedcontainers-2.4.0 trio-0.30.0 trio-websocket-0.12.2 webdriver-manager-4.0.2 websocket-client-1.8.0 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium webdriver-manager beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67064c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb Movie Review Scraper (Selenium Version)\n",
      "-------------------------------------------\n",
      "Note: This script requires Chrome and chromedriver to be installed.\n",
      "It will automatically download chromedriver if not already installed.\n",
      "Initial setup may take a moment.\n",
      "-------------------------------------------\n",
      "\n",
      "Searching for movie: Avatar\n",
      "Searching with URL: https://www.imdb.com/find/?q=Avatar\n",
      "\n",
      "Found the following movies:\n",
      "1. Avatar (2009) - tt0499549\n",
      "2. Avatar: The Last Airbender (2005) - tt0417299\n",
      "3. Avatar: Fire and Ash (2025) - tt1757678\n",
      "4. Avatar: The Way of Water (2022) - tt1630029\n",
      "5. Avatar: The Last Airbender (2024) - tt9018736\n",
      "\n",
      "Scraping reviews for: Avatar (tt0499549)\n",
      "Opening reviews page: https://www.imdb.com/title/tt0499549/reviews\n",
      "No more reviews to load: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00337FD3+60707]\n",
      "\tGetHandleVerifier [0x00338014+60772]\n",
      "\t(No symbol) [0x00160683]\n",
      "\t(No symbol) [0x001A8660]\n",
      "\t(No symbol) [0x001A89FB]\n",
      "\t(No symbol) [0x001F1022]\n",
      "\t(No symbol) [0x001CD094]\n",
      "\t(No symbol) [0x001EE824]\n",
      "\t(No symbol) [0x001CCE46]\n",
      "\t(No symbol) [0x0019C5D3]\n",
      "\t(No symbol) [0x0019D424]\n",
      "\tGetHandleVerifier [0x0057BAB3+2435075]\n",
      "\tGetHandleVerifier [0x00577053+2416035]\n",
      "\tGetHandleVerifier [0x005933FC+2531660]\n",
      "\tGetHandleVerifier [0x0034F0A5+155125]\n",
      "\tGetHandleVerifier [0x00355A4D+182173]\n",
      "\tGetHandleVerifier [0x0033F8A8+91640]\n",
      "\tGetHandleVerifier [0x0033FA50+92064]\n",
      "\tGetHandleVerifier [0x0032A510+4704]\n",
      "\tBaseThreadInitThunk [0x76EB7BA9+25]\n",
      "\tRtlInitializeExceptionChain [0x77DEC2EB+107]\n",
      "\tRtlClearBits [0x77DEC26F+191]\n",
      "\t(No symbol) [0x00000000]\n",
      "\n",
      "\n",
      "Found 0 reviews for Avatar\n",
      "\n",
      "Reviews saved to reviews/reviews_tt0499549_Avatar.json\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "def initialize_driver():\n",
    "    \"\"\"Initialize and return a Selenium WebDriver\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\")\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def search_movie_by_title(movie_title, driver):\n",
    "    \"\"\"Search for a movie by title using Selenium WebDriver\"\"\"\n",
    "    search_url = f\"https://www.imdb.com/find/?q={movie_title.replace(' ', '+')}\"\n",
    "    \n",
    "    print(f\"Searching with URL: {search_url}\")\n",
    "    \n",
    "    try:\n",
    "        # Visit the IMDb homepage first to get cookies\n",
    "        driver.get(\"https://www.imdb.com/\")\n",
    "        time.sleep(2)  # Let the page load\n",
    "        \n",
    "        # Now navigate to the search URL\n",
    "        driver.get(search_url)\n",
    "        time.sleep(3)  # Wait for search results to load\n",
    "        \n",
    "        # Wait for search results to appear\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \".ipc-metadata-list-summary-item\"))\n",
    "        )\n",
    "        \n",
    "        # Parse the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Find movie results\n",
    "        search_results = []\n",
    "        \n",
    "        # Try to find the results in the search page\n",
    "        result_items = soup.select('.ipc-metadata-list-summary-item')\n",
    "        \n",
    "        if not result_items:\n",
    "            # Try alternative search\n",
    "            driver.get(f\"https://www.imdb.com/search/title/?title={movie_title.replace(' ', '+')}\")\n",
    "            time.sleep(3)\n",
    "            \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            result_items = soup.select('.lister-item-content')\n",
    "        \n",
    "        for item in result_items:\n",
    "            try:\n",
    "                # Find the title link\n",
    "                link = item.select_one('a[href*=\"/title/tt\"]')\n",
    "                if not link:\n",
    "                    continue\n",
    "                \n",
    "                title = link.text.strip()\n",
    "                href = link.get('href', '')\n",
    "                \n",
    "                # Extract IMDb ID\n",
    "                imdb_id_match = re.search(r'/title/(tt\\d+)/', href)\n",
    "                if not imdb_id_match:\n",
    "                    continue\n",
    "                \n",
    "                imdb_id = imdb_id_match.group(1)\n",
    "                \n",
    "                # Try to extract year\n",
    "                year = \"Unknown\"\n",
    "                year_match = re.search(r'(\\d{4})', item.text)\n",
    "                if year_match:\n",
    "                    year = year_match.group(1)\n",
    "                \n",
    "                search_results.append({\n",
    "                    'title': title,\n",
    "                    'year': year,\n",
    "                    'imdb_id': imdb_id\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing result: {e}\")\n",
    "        \n",
    "        if not search_results:\n",
    "            # Direct search as a last resort\n",
    "            driver.get(f\"https://www.imdb.com/search/title/?title={movie_title.replace(' ', '+')}\")\n",
    "            time.sleep(3)\n",
    "            \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            \n",
    "            # Try to find movie items\n",
    "            items = soup.select('.lister-item')\n",
    "            \n",
    "            for item in items:\n",
    "                try:\n",
    "                    title_element = item.select_one('h3 a')\n",
    "                    if not title_element:\n",
    "                        continue\n",
    "                    \n",
    "                    title = title_element.text.strip()\n",
    "                    href = title_element.get('href', '')\n",
    "                    \n",
    "                    imdb_id_match = re.search(r'/title/(tt\\d+)/', href)\n",
    "                    if not imdb_id_match:\n",
    "                        continue\n",
    "                    \n",
    "                    imdb_id = imdb_id_match.group(1)\n",
    "                    \n",
    "                    year_element = item.select_one('.lister-item-year')\n",
    "                    year = \"Unknown\"\n",
    "                    if year_element:\n",
    "                        year_match = re.search(r'(\\d{4})', year_element.text)\n",
    "                        if year_match:\n",
    "                            year = year_match.group(1)\n",
    "                    \n",
    "                    search_results.append({\n",
    "                        'title': title,\n",
    "                        'year': year,\n",
    "                        'imdb_id': imdb_id\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing direct search result: {e}\")\n",
    "        \n",
    "        return search_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during search: {e}\")\n",
    "        return []\n",
    "\n",
    "def scrape_reviews_page(page_source, imdb_id):\n",
    "    \"\"\"Extract reviews from the page HTML\"\"\"\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "        reviews = soup.find_all('div', {'class': 'imdb-user-review'})\n",
    "    except:\n",
    "        reviews = []\n",
    "    \n",
    "    data = {}\n",
    "    data['ImdbId'] = imdb_id\n",
    "    reviews_text = []\n",
    "    \n",
    "    for review in reviews:\n",
    "        review_imdb = {}\n",
    "        \n",
    "        # Reviewer name\n",
    "        try:\n",
    "            review_imdb['reviewer_name'] = review.find('span', {'class': 'display-name-link'}).find('a').string.strip()\n",
    "        except:\n",
    "            review_imdb['reviewer_name'] = \"\"\n",
    "            \n",
    "        # Reviewer URL\n",
    "        try:\n",
    "            review_imdb['reviewer_url'] = review.find('span', {'class': 'display-name-link'}).find('a')['href']\n",
    "        except:\n",
    "            review_imdb['reviewer_url'] = \"\"\n",
    "            \n",
    "        # Review ID\n",
    "        try:\n",
    "            review_imdb['data-review-id'] = review['data-review-id']\n",
    "        except:\n",
    "            review_imdb['data-review-id'] = \"\"\n",
    "            \n",
    "        # Short review\n",
    "        try:\n",
    "            short_review = review.find('a', {'class': 'title'})\n",
    "            text = short_review.string.strip()\n",
    "            review_imdb['short_review'] = text\n",
    "        except:\n",
    "            review_imdb['short_review'] = \"\"\n",
    "    \n",
    "        # Full review\n",
    "        try:\n",
    "            full_review = review.find('div', {'class': 'show-more__control'})\n",
    "            text = full_review.string.strip()\n",
    "            review_imdb['full_review'] = text\n",
    "        except:\n",
    "            review_imdb['full_review'] = \"\"\n",
    "            \n",
    "        # Review date\n",
    "        try:\n",
    "            review_date = review.find('span', {'class': 'review-date'})\n",
    "            text = review_date.string.strip()\n",
    "            review_imdb['review_date'] = text    \n",
    "        except:\n",
    "            review_imdb['review_date'] = \"\" \n",
    "            \n",
    "        # Rating value\n",
    "        try:\n",
    "            ratings_span = review.find('span', {'class': 'rating-other-user-rating'})\n",
    "            text = ratings_span.find('span').string.strip()\n",
    "            review_imdb['rating_value'] = text      \n",
    "        except:\n",
    "            review_imdb['rating_value'] = \"\" \n",
    "            \n",
    "        reviews_text.append(review_imdb)    \n",
    "    \n",
    "    data['reviews'] = reviews_text\n",
    "    return data\n",
    "\n",
    "def scrape_all_reviews(imdb_id, driver, max_pages=None):\n",
    "    \"\"\"Scrape all review pages for a movie using Selenium\"\"\"\n",
    "    all_data = []\n",
    "    reviews_url = f\"https://www.imdb.com/title/{imdb_id}/reviews\"\n",
    "    \n",
    "    print(f\"Opening reviews page: {reviews_url}\")\n",
    "    driver.get(reviews_url)\n",
    "    time.sleep(3)  # Wait for page to load\n",
    "    \n",
    "    page_count = 0\n",
    "    has_more = True\n",
    "    \n",
    "    # First parse the initial page\n",
    "    data = scrape_reviews_page(driver.page_source, imdb_id)\n",
    "    if data['reviews']:\n",
    "        all_data.append(data)\n",
    "        print(f\"Found {len(data['reviews'])} reviews on page {page_count + 1}\")\n",
    "    \n",
    "    # Click \"Load More\" button until no more results or reached max pages\n",
    "    while has_more and (max_pages is None or page_count < max_pages):\n",
    "        try:\n",
    "            # Wait for the \"Load More\" button to be clickable\n",
    "            load_more = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.CLASS_NAME, \"ipl-load-more__button\"))\n",
    "            )\n",
    "            \n",
    "            # Scroll to the button to make sure it's visible\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", load_more)\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Click the button\n",
    "            load_more.click()\n",
    "            \n",
    "            # Wait for new content to load\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Parse the updated page\n",
    "            page_count += 1\n",
    "            print(f\"Loading more reviews (page {page_count + 1})...\")\n",
    "            \n",
    "            data = scrape_reviews_page(driver.page_source, imdb_id)\n",
    "            if data['reviews']:\n",
    "                all_data.append(data)\n",
    "                print(f\"Found {len(data['reviews'])} additional reviews\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"No more reviews to load: {e}\")\n",
    "            has_more = False\n",
    "    \n",
    "    reviews = {}\n",
    "    reviews['ImdbId'] = imdb_id\n",
    "    reviews['reviews'] = all_data\n",
    "    return reviews\n",
    "\n",
    "def get_movie_reviews_by_title(movie_title, max_pages=None):\n",
    "    \"\"\"Main function to get reviews by movie title using Selenium\"\"\"\n",
    "    print(f\"\\nSearching for movie: {movie_title}\")\n",
    "    \n",
    "    driver = initialize_driver()\n",
    "    \n",
    "    try:\n",
    "        # Search for the movie\n",
    "        search_results = search_movie_by_title(movie_title, driver)\n",
    "        \n",
    "        if not search_results:\n",
    "            print(\"No movies found matching that title.\")\n",
    "            driver.quit()\n",
    "            return None\n",
    "        \n",
    "        # Display search results\n",
    "        print(\"\\nFound the following movies:\")\n",
    "        for i, movie in enumerate(search_results, 1):\n",
    "            print(f\"{i}. {movie['title']} ({movie['year']}) - {movie['imdb_id']}\")\n",
    "        \n",
    "        # Let user choose a movie\n",
    "        choice = 0\n",
    "        while choice < 1 or choice > len(search_results):\n",
    "            try:\n",
    "                choice = int(input(f\"\\nSelect a movie (1-{len(search_results)}): \"))\n",
    "            except ValueError:\n",
    "                print(\"Please enter a valid number.\")\n",
    "        \n",
    "        selected_movie = search_results[choice-1]\n",
    "        imdb_id = selected_movie['imdb_id']\n",
    "        movie_title = selected_movie['title']\n",
    "        \n",
    "        print(f\"\\nScraping reviews for: {movie_title} ({imdb_id})\")\n",
    "        \n",
    "        # Scrape reviews for the selected movie\n",
    "        data = scrape_all_reviews(imdb_id, driver, max_pages)\n",
    "        \n",
    "        # Count total reviews across all pages\n",
    "        total_reviews = 0\n",
    "        for page in data['reviews']:\n",
    "            total_reviews += len(page['reviews'])\n",
    "        \n",
    "        print(f\"\\nFound {total_reviews} reviews for {movie_title}\")\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(\"reviews\", exist_ok=True)\n",
    "        \n",
    "        # Save to JSON file\n",
    "        sanitized_title = re.sub(r'[\\\\/*?:\"<>|]', \"\", movie_title.replace(' ', '_'))\n",
    "        filename = f\"reviews/reviews_{imdb_id}_{sanitized_title}.json\"\n",
    "        with open(filename, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(f\"\\nReviews saved to {filename}\")\n",
    "        return data\n",
    "        \n",
    "    finally:\n",
    "        # Always close the driver when done\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"IMDb Movie Review Scraper (Selenium Version)\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"Note: This script requires Chrome and chromedriver to be installed.\")\n",
    "    print(\"It will automatically download chromedriver if not already installed.\")\n",
    "    print(\"Initial setup may take a moment.\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    \n",
    "    while True:\n",
    "        movie_title = input(\"\\nEnter movie title (or 'quit' to exit): \")\n",
    "        if movie_title.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        max_pages = None\n",
    "        page_limit = input(\"Enter maximum number of pages to scrape (or press Enter for all): \")\n",
    "        if page_limit.strip():\n",
    "            try:\n",
    "                max_pages = int(page_limit)\n",
    "            except ValueError:\n",
    "                print(\"Invalid number, scraping all pages.\")\n",
    "        \n",
    "        get_movie_reviews_by_title(movie_title, max_pages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
